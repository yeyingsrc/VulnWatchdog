# VulnWatchdog åç»­å¼€å‘è®¡åˆ’

**æ›´æ–°æ—¶é—´**: 2025-11-27
**å½“å‰ç‰ˆæœ¬**: v2.0
**è§„åˆ’å‘¨æœŸ**: 2025 Q4 - 2026 Q2

---

## ğŸ“Š å½“å‰çŠ¶æ€è¯„ä¼°

### âœ… å·²å®ŒæˆåŠŸèƒ½

| æ¨¡å— | åŠŸèƒ½ | çŠ¶æ€ | ç‰ˆæœ¬ |
|------|------|------|------|
| **æ ¸å¿ƒåŠŸèƒ½** | GitHub CVEä»“åº“æœç´¢ | âœ… | v1.0 |
| **æ ¸å¿ƒåŠŸèƒ½** | CVEä¿¡æ¯è·å– | âœ… | v1.0 |
| **æ ¸å¿ƒåŠŸèƒ½** | GPTæ¼æ´åˆ†æ | âœ… | v1.0 |
| **æ ¸å¿ƒåŠŸèƒ½** | POCä»£ç æå– | âœ… | v1.0 |
| **æ ¸å¿ƒåŠŸèƒ½** | MarkdownæŠ¥å‘Šç”Ÿæˆ | âœ… | v1.0 |
| **æ ¸å¿ƒåŠŸèƒ½** | é€šçŸ¥æ¨é€ï¼ˆé£ä¹¦ï¼‰ | âœ… | v1.0 |
| **ä¼˜åŒ–-P1** | ç²¾ç¡®æ›´æ–°æ£€æµ‹ | âœ… | v1.5 |
| **ä¼˜åŒ–-P1** | ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç† | âœ… | v1.5 |
| **ä¼˜åŒ–-P1** | GPTè§£æå¢å¼º | âœ… | v1.5 |
| **ä¼˜åŒ–-P2** | POCæå–æ•ˆç‡ä¼˜åŒ– | âœ… | v2.0 |
| **ä¼˜åŒ–-P2** | GitHub Tokenè®¤è¯ | âœ… | v2.0 |
| **ä¼˜åŒ–-P2** | ç›‘æ§å‘Šè­¦åŸºç¡€ | âœ… | v2.0 |

### ğŸ” å¾…æ”¹è¿›çš„é—®é¢˜

| é—®é¢˜ | å½±å“ | ä¼˜å…ˆçº§ |
|------|------|--------|
| ç›‘æ§æœªé›†æˆåˆ°ä¸»æµç¨‹ | æ— æ³•å®æ—¶æŸ¥çœ‹è¿è¡ŒçŠ¶æ€ | P0 |
| ç¼ºå°‘å¹¶å‘å¤„ç† | å¤„ç†é€Ÿåº¦è¾ƒæ…¢ | P0 |
| å•ä¸€é€šçŸ¥æ¸ é“ï¼ˆä»…é£ä¹¦ï¼‰ | çµæ´»æ€§ä¸è¶³ | P1 |
| ç¼ºå°‘æ•°æ®å¯è§†åŒ– | éš¾ä»¥åˆ†æè¶‹åŠ¿ | P1 |
| ç¼ºå°‘å¢é‡æ›´æ–°æœºåˆ¶ | é‡å¤å¤„ç†æ•°æ® | P1 |
| ç¼ºå°‘ç¼“å­˜æœºåˆ¶ | APIè°ƒç”¨æµªè´¹ | P2 |
| ç¼ºå°‘å¼‚å¸¸æ¢å¤ | ä¸­æ–­åéœ€é‡è·‘ | P1 |
| ç¼ºå°‘æ•°æ®å¯¼å‡º | æ•°æ®åˆ©ç”¨ç‡ä½ | P2 |

---

## ğŸ¯ ç¬¬ä¸‰é˜¶æ®µä¼˜åŒ– (P3) - æ€§èƒ½ä¸ç¨³å®šæ€§

**ç›®æ ‡**: æå‡å¤„ç†é€Ÿåº¦ã€ç³»ç»Ÿç¨³å®šæ€§å’Œå¯é æ€§
**æ—¶é—´**: 2å‘¨
**ä¼˜å…ˆçº§**: P0

### 3.1 ç›‘æ§é›†æˆåˆ°ä¸»æµç¨‹ â­â­â­â­â­

**é—®é¢˜**:
- ç›‘æ§æ¨¡å—å·²å¼€å‘å®Œæˆï¼Œä½†æœªé›†æˆåˆ°main.py
- æ— æ³•å®æ—¶æŸ¥çœ‹è¿è¡ŒæŒ‡æ ‡

**æ–¹æ¡ˆ**:

```python
# main.py æ”¹é€ 
from libs.monitor import get_monitor

def main():
    monitor = get_monitor()

    try:
        # æœç´¢é˜¶æ®µ
        cve_list, repo_list = search_github(query)
        monitor.record_cve_found(len(cve_list))
        monitor.record_repo_found(len(repo_list))

        # å¤„ç†é˜¶æ®µ
        for repo in repo_list:
            try:
                # æ›´æ–°æ£€æµ‹
                if enable_update_check:
                    latest_sha = get_latest_commit_sha(repo_url)
                    monitor.record_github_api_call(success=True)
                    monitor.record_update_check(has_update=...)

                # GPTåˆ†æ
                if enable_gpt:
                    result = ask_gpt(prompt)
                    monitor.record_gpt_call(success=result is not None)

                # å…‹éš†
                clone_path = __clone_repo(repo_url)
                monitor.record_clone(success=clone_path is not None)
                monitor.record_temp_dir_created()

                # æ¸…ç†
                monitor.record_temp_dir_cleaned(success=True)

                # è®°å½•ç»“æœ
                if action == 'new':
                    monitor.record_repo_new()
                elif action == 'update':
                    monitor.record_repo_updated()
                else:
                    monitor.record_repo_skipped()

            except Exception as e:
                monitor.record_error('processing_error', str(e), {'repo': repo_url})
                monitor.record_repo_failed()

    finally:
        # æ‰“å°æ‘˜è¦
        monitor.print_summary()
        # ä¿å­˜åˆ°æ–‡ä»¶
        monitor.save_to_file('logs/monitor_report.json')
        # å¥åº·æ£€æŸ¥
        health = monitor.check_health()
        if health['status'] != 'healthy':
            logger.warning(f"å¥åº·çŠ¶æ€: {health['status']}, è­¦å‘Š: {health['warnings']}")
```

**æ”¹åŠ¨æ–‡ä»¶**:
- `main.py` - é›†æˆç›‘æ§è°ƒç”¨

**é¢„æœŸæ•ˆæœ**:
- âœ… å®æ—¶ç›‘æ§è¿è¡ŒçŠ¶æ€
- âœ… è‡ªåŠ¨ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š
- âœ… å¥åº·çŠ¶æ€è‡ªåŠ¨æ£€æŸ¥
- âœ… å†å²æ•°æ®å¯è¿½æº¯ï¼ˆJSONæ–‡ä»¶ï¼‰

**å·¥ä½œé‡**: 2å°æ—¶

---

### 3.2 å¹¶å‘å¤„ç†ä¼˜åŒ– â­â­â­â­â­

**é—®é¢˜**:
- å½“å‰ä¸²è¡Œå¤„ç†ï¼Œå¤„ç†7680ä¸ªä»“åº“è€—æ—¶è¿‡é•¿
- CPUå’Œç½‘ç»œèµ„æºåˆ©ç”¨ç‡ä½

**æ–¹æ¡ˆ**:

#### æ–¹æ¡ˆA: å¤šçº¿ç¨‹ (æ¨è)

```python
# main.py
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# çº¿ç¨‹å®‰å…¨çš„ç›‘æ§
monitor_lock = threading.Lock()

def process_repo_with_monitor(repo_data):
    """çº¿ç¨‹å®‰å…¨çš„ä»“åº“å¤„ç†"""
    result = process_repository(repo_data)

    # çº¿ç¨‹å®‰å…¨è®°å½•
    with monitor_lock:
        if result['action'] == 'new':
            monitor.record_repo_new()
        elif result['action'] == 'update':
            monitor.record_repo_updated()
        # ...

    return result

def main():
    # å¹¶å‘å¤„ç†
    max_workers = 10  # æ ¹æ®ç³»ç»Ÿé…ç½®è°ƒæ•´

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤ä»»åŠ¡
        futures = {
            executor.submit(process_repo_with_monitor, repo): repo
            for repo in repo_list
        }

        # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦ï¼‰
        for idx, future in enumerate(as_completed(futures), 1):
            try:
                result = future.result()
                logger.info(f"è¿›åº¦: {idx}/{len(repo_list)} - {result['cve_id']}")
            except Exception as e:
                repo = futures[future]
                logger.error(f"å¤„ç†å¤±è´¥: {repo['cve_id']} - {e}")
```

**æ”¹åŠ¨æ–‡ä»¶**:
- `main.py` - æ·»åŠ å¹¶å‘å¤„ç†é€»è¾‘
- `libs/utils.py` - ç¡®ä¿çº¿ç¨‹å®‰å…¨

**é¢„æœŸæ•ˆæœ**:
- âœ… å¤„ç†é€Ÿåº¦æå‡ **5-10å€**
- âœ… 7680ä¸ªä»“åº“ï¼š1.5å°æ—¶ â†’ 15åˆ†é’Ÿ
- âœ… èµ„æºåˆ©ç”¨ç‡æå‡

**é…ç½®é¡¹**:
```python
# config.py
MAX_WORKERS = 10  # å¹¶å‘æ•°ï¼ˆå»ºè®®5-20ï¼‰
```

**æ³¨æ„äº‹é¡¹**:
- GitHub APIé™æµï¼ˆéœ€Tokenè®¤è¯ï¼‰
- æ•°æ®åº“å†™å…¥å†²çªï¼ˆéœ€åŠ é”æˆ–ä½¿ç”¨è¿æ¥æ± ï¼‰
- GPT APIé™æµï¼ˆéœ€æ§åˆ¶å¹¶å‘ï¼‰

**å·¥ä½œé‡**: 1å¤©

---

### 3.3 å¼‚å¸¸æ¢å¤ä¸æ–­ç‚¹ç»­ä¼  â­â­â­â­

**é—®é¢˜**:
- ç¨‹åºä¸­æ–­åéœ€è¦ä»å¤´å¼€å§‹
- ç½‘ç»œå¼‚å¸¸å¯¼è‡´é‡å¤å¤„ç†

**æ–¹æ¡ˆ**:

```python
# è¿›åº¦è·Ÿè¸ª
class ProgressTracker:
    def __init__(self, checkpoint_file='progress.json'):
        self.checkpoint_file = checkpoint_file
        self.processed_repos = self._load_checkpoint()

    def _load_checkpoint(self):
        """åŠ è½½å·²å¤„ç†è®°å½•"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file) as f:
                return set(json.load(f))
        return set()

    def mark_processed(self, repo_url):
        """æ ‡è®°å·²å¤„ç†"""
        self.processed_repos.add(repo_url)
        self._save_checkpoint()

    def is_processed(self, repo_url):
        """æ£€æŸ¥æ˜¯å¦å·²å¤„ç†"""
        return repo_url in self.processed_repos

    def _save_checkpoint(self):
        """ä¿å­˜è¿›åº¦"""
        with open(self.checkpoint_file, 'w') as f:
            json.dump(list(self.processed_repos), f)

    def clear(self):
        """æ¸…é™¤è¿›åº¦ï¼ˆæ–°ä¸€è½®è¿è¡Œï¼‰"""
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)

# main.py ä½¿ç”¨
def main():
    tracker = ProgressTracker()

    # æ˜¯å¦æ˜¯æ–°ä¸€è½®è¿è¡Œ
    if is_new_run():
        tracker.clear()

    for repo in repo_list:
        # è·³è¿‡å·²å¤„ç†
        if tracker.is_processed(repo['html_url']):
            logger.info(f"è·³è¿‡å·²å¤„ç†ä»“åº“: {repo['html_url']}")
            continue

        try:
            result = process_repository(repo)
            tracker.mark_processed(repo['html_url'])
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            # ä¸æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œä¸‹æ¬¡é‡è¯•
```

**æ”¹åŠ¨æ–‡ä»¶**:
- `libs/progress.py` - æ–°å¢è¿›åº¦è·Ÿè¸ªæ¨¡å—
- `main.py` - é›†æˆæ–­ç‚¹ç»­ä¼ 

**é¢„æœŸæ•ˆæœ**:
- âœ… ä¸­æ–­åå¯ç»§ç»­è¿è¡Œ
- âœ… é¿å…é‡å¤å¤„ç†
- âœ… æå‡ç¨³å®šæ€§

**å·¥ä½œé‡**: 4å°æ—¶

---

### 3.4 å¢é‡æ›´æ–°æœºåˆ¶ â­â­â­â­

**é—®é¢˜**:
- æ¯æ¬¡éƒ½æœç´¢å…¨éƒ¨ä»“åº“
- å·²å¤„ç†çš„è€ä»“åº“é‡å¤æ£€æŸ¥

**æ–¹æ¡ˆ**:

```python
# config.py
INCREMENTAL_MODE = True  # å¯ç”¨å¢é‡æ¨¡å¼
INCREMENTAL_DAYS = 7     # åªå¤„ç†æœ€è¿‘7å¤©æ›´æ–°çš„ä»“åº“

# main.py
def is_recently_updated(repo_data, days=7):
    """æ£€æŸ¥ä»“åº“æ˜¯å¦æœ€è¿‘æ›´æ–°"""
    updated_at = repo_data.get('updated_at')
    if not updated_at:
        return True  # ä¿é™©èµ·è§ï¼ŒåŒ…å«æœªçŸ¥æ—¶é—´çš„

    update_time = datetime.strptime(updated_at, '%Y-%m-%dT%H:%M:%SZ')
    cutoff_time = datetime.now() - timedelta(days=days)
    return update_time >= cutoff_time

def main():
    if get_config('INCREMENTAL_MODE'):
        days = get_config('INCREMENTAL_DAYS')
        logger.info(f"å¢é‡æ¨¡å¼ï¼šä»…å¤„ç†æœ€è¿‘{days}å¤©æ›´æ–°çš„ä»“åº“")

        # è¿‡æ»¤ä»“åº“
        repo_list = [
            repo for repo in all_repos
            if is_recently_updated(repo, days)
        ]
        logger.info(f"è¿‡æ»¤åä»“åº“æ•°: {len(repo_list)}")
```

**æ”¹åŠ¨æ–‡ä»¶**:
- `config.py` - å¢é‡é…ç½®
- `main.py` - å¢é‡é€»è¾‘

**é¢„æœŸæ•ˆæœ**:
- âœ… å‡å°‘å¤„ç†é‡ 70-90%
- âœ… è¿è¡Œæ—¶é—´ç¼©çŸ­
- âœ… APIè°ƒç”¨å‡å°‘

**å·¥ä½œé‡**: 2å°æ—¶

---

## ğŸš€ ç¬¬å››é˜¶æ®µä¼˜åŒ– (P4) - åŠŸèƒ½æ‰©å±•

**ç›®æ ‡**: å¢å¼ºç³»ç»ŸåŠŸèƒ½å’Œç”¨æˆ·ä½“éªŒ
**æ—¶é—´**: 3å‘¨
**ä¼˜å…ˆçº§**: P1

### 4.1 å¤šæ¸ é“é€šçŸ¥æ”¯æŒ â­â­â­â­

**é—®é¢˜**:
- å½“å‰ä»…æ”¯æŒé£ä¹¦
- ä¼ä¸šå¯èƒ½ä½¿ç”¨å…¶ä»–å·¥å…·

**æ–¹æ¡ˆ**:

æ–°å¢é€šçŸ¥æ¸ é“ï¼š
1. **é’‰é’‰** (DingTalk)
2. **ä¼ä¸šå¾®ä¿¡** (WeChat Work)
3. **Slack**
4. **Email**
5. **Webhook** (é€šç”¨)
6. **Telegram**

```python
# libs/notifier.py (é‡æ„)
class NotifierFactory:
    @staticmethod
    def create(notifier_type: str):
        if notifier_type == 'feishu':
            return FeishuNotifier()
        elif notifier_type == 'dingtalk':
            return DingTalkNotifier()
        elif notifier_type == 'wechat':
            return WeChatNotifier()
        elif notifier_type == 'slack':
            return SlackNotifier()
        elif notifier_type == 'email':
            return EmailNotifier()
        else:
            raise ValueError(f"æœªçŸ¥é€šçŸ¥ç±»å‹: {notifier_type}")

# æ”¯æŒå¤šé€šé“
# config.py
NOTIFY_CHANNELS = ['feishu', 'email']  # å¤šé€šé“é€šçŸ¥

# main.py
for channel in get_config('NOTIFY_CHANNELS'):
    notifier = NotifierFactory.create(channel)
    notifier.send(data)
```

**æ–°å¢æ–‡ä»¶**:
- `libs/notifiers/base.py` - é€šçŸ¥åŸºç±»
- `libs/notifiers/feishu.py` - é£ä¹¦
- `libs/notifiers/dingtalk.py` - é’‰é’‰
- `libs/notifiers/wechat.py` - ä¼ä¸šå¾®ä¿¡
- `libs/notifiers/slack.py` - Slack
- `libs/notifiers/email.py` - é‚®ä»¶
- `template/dingtalk.json`
- `template/wechat.json`
- `template/slack.json`

**å·¥ä½œé‡**: 3å¤©

---

### 4.2 æ•°æ®å¯è§†åŒ–é¢æ¿ â­â­â­â­

**é—®é¢˜**:
- æ•°æ®å­˜å‚¨åœ¨æ•°æ®åº“ï¼Œç¼ºå°‘ç›´è§‚å±•ç¤º
- éš¾ä»¥åˆ†æè¶‹åŠ¿å’Œç»Ÿè®¡

**æ–¹æ¡ˆ**:

#### æ–¹æ¡ˆA: Webä»ªè¡¨ç›˜ (æ¨è)

ä½¿ç”¨ **Streamlit** æˆ– **Dash** å¿«é€Ÿæ­å»ºï¼š

```python
# dashboard.py
import streamlit as st
import pandas as pd
import plotly.express as px
from models.models import get_db, CVE, Repository

st.set_page_config(page_title="VulnWatchdog Dashboard", layout="wide")

# æ ‡é¢˜
st.title("ğŸ• VulnWatchdog ç›‘æ§é¢æ¿")

# æ•°æ®åŠ è½½
@st.cache_data(ttl=600)  # ç¼“å­˜10åˆ†é’Ÿ
def load_data():
    db = next(get_db())
    cves = db.query(CVE).all()
    repos = db.query(Repository).all()
    return cves, repos

cves, repos = load_data()

# ç»Ÿè®¡å¡ç‰‡
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("CVEæ€»æ•°", len(cves))
with col2:
    st.metric("ä»“åº“æ€»æ•°", len(repos))
with col3:
    new_count = len([r for r in repos if r.created_at > datetime.now() - timedelta(days=7)])
    st.metric("æœ¬å‘¨æ–°å¢", new_count)
with col4:
    high_risk = len([c for c in cves if c.cvss_score >= 7.0])
    st.metric("é«˜å±æ¼æ´", high_risk)

# å›¾è¡¨1: CVEå¹´ä»½åˆ†å¸ƒ
st.subheader("ğŸ“Š CVEå¹´ä»½åˆ†å¸ƒ")
cve_years = [cve.id.split('-')[1] for cve in cves]
year_counts = pd.Series(cve_years).value_counts().sort_index()
fig1 = px.bar(x=year_counts.index, y=year_counts.values, labels={'x': 'å¹´ä»½', 'y': 'æ•°é‡'})
st.plotly_chart(fig1, use_container_width=True)

# å›¾è¡¨2: é£é™©ç­‰çº§åˆ†å¸ƒ
st.subheader("âš ï¸ é£é™©ç­‰çº§åˆ†å¸ƒ")
risk_levels = [cve.cvss_score for cve in cves if cve.cvss_score]
risk_df = pd.DataFrame({
    'level': ['ä½å±', 'ä¸­å±', 'é«˜å±', 'ä¸¥é‡'],
    'count': [
        len([s for s in risk_levels if s < 4.0]),
        len([s for s in risk_levels if 4.0 <= s < 7.0]),
        len([s for s in risk_levels if 7.0 <= s < 9.0]),
        len([s for s in risk_levels if s >= 9.0])
    ]
})
fig2 = px.pie(risk_df, names='level', values='count')
st.plotly_chart(fig2, use_container_width=True)

# å›¾è¡¨3: æ¯æ—¥æ–°å¢è¶‹åŠ¿
st.subheader("ğŸ“ˆ æ¯æ—¥æ–°å¢è¶‹åŠ¿")
daily_data = pd.DataFrame([
    {'date': r.created_at.date(), 'count': 1}
    for r in repos if r.created_at
])
daily_counts = daily_data.groupby('date').count().reset_index()
fig3 = px.line(daily_counts, x='date', y='count', labels={'date': 'æ—¥æœŸ', 'count': 'æ–°å¢æ•°é‡'})
st.plotly_chart(fig3, use_container_width=True)

# æ•°æ®è¡¨æ ¼
st.subheader("ğŸ“‹ æœ€è¿‘ä»“åº“")
recent_repos = sorted(repos, key=lambda r: r.created_at, reverse=True)[:20]
st.dataframe([
    {
        'CVE': r.cve_id,
        'ä»“åº“': r.url,
        'å‘ç°æ—¶é—´': r.created_at.strftime('%Y-%m-%d %H:%M'),
        'æ›´æ–°æ—¶é—´': r.updated_at.strftime('%Y-%m-%d %H:%M') if r.updated_at else '-'
    }
    for r in recent_repos
])
```

**è¿è¡Œ**:
```bash
streamlit run dashboard.py
# è®¿é—® http://localhost:8501
```

**æ–°å¢ä¾èµ–**:
```bash
pip install streamlit plotly pandas
```

**å·¥ä½œé‡**: 2å¤©

---

### 4.3 APIæ¥å£å¼€å‘ â­â­â­

**é—®é¢˜**:
- æ•°æ®æ— æ³•è¢«å…¶ä»–ç³»ç»Ÿè®¿é—®
- ç¼ºå°‘ç¨‹åºåŒ–æŸ¥è¯¢æ–¹å¼

**æ–¹æ¡ˆ**:

ä½¿ç”¨ **FastAPI** æä¾›RESTful APIï¼š

```python
# api.py
from fastapi import FastAPI, Query, HTTPException
from typing import List, Optional
from models.models import get_db, CVE, Repository
from pydantic import BaseModel

app = FastAPI(title="VulnWatchdog API", version="2.0")

class CVEResponse(BaseModel):
    id: str
    title: str
    cvss_score: Optional[float]
    description: Optional[str]

    class Config:
        from_attributes = True

@app.get("/api/cves", response_model=List[CVEResponse])
async def list_cves(
    year: Optional[int] = None,
    min_score: Optional[float] = None,
    limit: int = Query(100, le=1000)
):
    """è·å–CVEåˆ—è¡¨"""
    db = next(get_db())
    query = db.query(CVE)

    if year:
        query = query.filter(CVE.id.like(f'CVE-{year}-%'))
    if min_score:
        query = query.filter(CVE.cvss_score >= min_score)

    return query.limit(limit).all()

@app.get("/api/cves/{cve_id}")
async def get_cve(cve_id: str):
    """è·å–CVEè¯¦æƒ…"""
    db = next(get_db())
    cve = db.query(CVE).filter(CVE.id == cve_id).first()
    if not cve:
        raise HTTPException(status_code=404, detail="CVE not found")
    return cve

@app.get("/api/repositories")
async def list_repos(cve_id: Optional[str] = None, limit: int = 100):
    """è·å–ä»“åº“åˆ—è¡¨"""
    db = next(get_db())
    query = db.query(Repository)

    if cve_id:
        query = query.filter(Repository.cve_id == cve_id)

    return query.limit(limit).all()

@app.get("/api/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    db = next(get_db())
    return {
        'total_cves': db.query(CVE).count(),
        'total_repos': db.query(Repository).count(),
        'recent_cves': db.query(CVE).filter(
            CVE.created_at > datetime.now() - timedelta(days=7)
        ).count()
    }
```

**è¿è¡Œ**:
```bash
uvicorn api:app --reload
# è®¿é—® http://localhost:8000/docs (Swagger UI)
```

**æ–°å¢ä¾èµ–**:
```bash
pip install fastapi uvicorn[standard]
```

**å·¥ä½œé‡**: 1å¤©

---

### 4.4 æ•°æ®å¯¼å‡ºåŠŸèƒ½ â­â­â­

**é—®é¢˜**:
- æ•°æ®é”å®šåœ¨æ•°æ®åº“
- æ— æ³•å¯¼å‡ºç”¨äºåˆ†æ

**æ–¹æ¡ˆ**:

```python
# scripts/export_data.py
import csv
import json
import argparse
from models.models import get_db, CVE, Repository

def export_to_csv(output_file='export.csv'):
    """å¯¼å‡ºä¸ºCSV"""
    db = next(get_db())
    repos = db.query(Repository).all()

    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=[
            'cve_id', 'repo_url', 'stars', 'created_at', 'updated_at',
            'risk_level', 'cvss_score'
        ])
        writer.writeheader()

        for repo in repos:
            writer.writerow({
                'cve_id': repo.cve_id,
                'repo_url': repo.url,
                'stars': repo.stars,
                'created_at': repo.created_at,
                'updated_at': repo.updated_at,
                'risk_level': repo.gpt_analysis.get('risk') if repo.gpt_analysis else '',
                'cvss_score': repo.cve.cvss_score if repo.cve else ''
            })

def export_to_json(output_file='export.json'):
    """å¯¼å‡ºä¸ºJSON"""
    db = next(get_db())
    repos = db.query(Repository).all()

    data = []
    for repo in repos:
        data.append({
            'cve_id': repo.cve_id,
            'repo_url': repo.url,
            'gpt_analysis': repo.gpt_analysis,
            'created_at': repo.created_at.isoformat(),
            'updated_at': repo.updated_at.isoformat() if repo.updated_at else None
        })

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--format', choices=['csv', 'json'], default='csv')
    parser.add_argument('--output', default='export')
    args = parser.parse_args()

    output_file = f"{args.output}.{args.format}"

    if args.format == 'csv':
        export_to_csv(output_file)
    else:
        export_to_json(output_file)

    print(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
```

**ä½¿ç”¨**:
```bash
# å¯¼å‡ºCSV
python scripts/export_data.py --format csv --output data_export

# å¯¼å‡ºJSON
python scripts/export_data.py --format json --output data_export
```

**å·¥ä½œé‡**: 4å°æ—¶

---

## ğŸ”§ ç¬¬äº”é˜¶æ®µä¼˜åŒ– (P5) - æ€§èƒ½è°ƒä¼˜

**ç›®æ ‡**: è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½å’Œæˆæœ¬
**æ—¶é—´**: 2å‘¨
**ä¼˜å…ˆçº§**: P2

### 5.1 ç¼“å­˜æœºåˆ¶ â­â­â­

**é—®é¢˜**:
- CVEä¿¡æ¯é‡å¤æŸ¥è¯¢
- GitHub APIé‡å¤è°ƒç”¨

**æ–¹æ¡ˆ**:

```python
# libs/cache.py
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path

class SimpleCache:
    def __init__(self, cache_dir='.cache', ttl=3600):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.ttl = ttl  # ç§’

    def _get_cache_path(self, key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        key_hash = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{key_hash}.json"

    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        cache_file = self._get_cache_path(key)

        if not cache_file.exists():
            return None

        # æ£€æŸ¥è¿‡æœŸ
        with open(cache_file) as f:
            cache_data = json.load(f)

        cached_time = datetime.fromisoformat(cache_data['timestamp'])
        if datetime.now() - cached_time > timedelta(seconds=self.ttl):
            cache_file.unlink()  # åˆ é™¤è¿‡æœŸç¼“å­˜
            return None

        return cache_data['value']

    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self._get_cache_path(key)
        cache_data = {
            'timestamp': datetime.now().isoformat(),
            'value': value
        }

        with open(cache_file, 'w') as f:
            json.dump(cache_data, f)

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        for cache_file in self.cache_dir.glob('*.json'):
            cache_file.unlink()

# libs/utils.py ä½¿ç”¨ç¼“å­˜
from libs.cache import SimpleCache

cve_cache = SimpleCache(cache_dir='.cache/cve', ttl=86400)  # 24å°æ—¶
github_cache = SimpleCache(cache_dir='.cache/github', ttl=3600)  # 1å°æ—¶

def get_cve_info(cve_id: str) -> Dict:
    # å…ˆæŸ¥ç¼“å­˜
    cached = cve_cache.get(cve_id)
    if cached:
        logger.debug(f"å‘½ä¸­CVEç¼“å­˜: {cve_id}")
        return cached

    # æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢API
    data = _fetch_cve_from_api(cve_id)

    # å­˜å…¥ç¼“å­˜
    if data:
        cve_cache.set(cve_id, data)

    return data
```

**é…ç½®**:
```python
# config.py
CACHE_ENABLED = True
CACHE_TTL_CVE = 86400      # CVEä¿¡æ¯ç¼“å­˜24å°æ—¶
CACHE_TTL_GITHUB = 3600    # GitHubä¿¡æ¯ç¼“å­˜1å°æ—¶
```

**é¢„æœŸæ•ˆæœ**:
- âœ… APIè°ƒç”¨å‡å°‘ 50-80%
- âœ… å“åº”é€Ÿåº¦æå‡
- âœ… æˆæœ¬é™ä½

**å·¥ä½œé‡**: 1å¤©

---

### 5.2 æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– â­â­â­

**é—®é¢˜**:
- æŸ¥è¯¢é€Ÿåº¦æ…¢ï¼ˆ7680æ¡è®°å½•ï¼‰
- ç¼ºå°‘åˆé€‚ç´¢å¼•

**æ–¹æ¡ˆ**:

```python
# models/models.py
from sqlalchemy import Index

class Repository(Base):
    __tablename__ = 'repositories'

    id = Column(Integer, primary_key=True)
    cve_id = Column(String(50), nullable=False, index=True)  # æ·»åŠ ç´¢å¼•
    url = Column(String(255), unique=True, index=True)       # æ·»åŠ ç´¢å¼•
    latest_commit_sha = Column(String(40), index=True)       # æ·»åŠ ç´¢å¼•
    created_at = Column(DateTime, default=datetime.now, index=True)  # æ·»åŠ ç´¢å¼•
    updated_at = Column(DateTime, onupdate=datetime.now, index=True) # æ·»åŠ ç´¢å¼•

    # å¤åˆç´¢å¼•
    __table_args__ = (
        Index('idx_cve_created', 'cve_id', 'created_at'),
        Index('idx_url_sha', 'url', 'latest_commit_sha'),
    )

# è¿ç§»è„šæœ¬
# scripts/add_indexes.py
from models.models import engine
from sqlalchemy import text

def add_indexes():
    with engine.connect() as conn:
        # æ·»åŠ ç´¢å¼•
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_repo_cve_id ON repositories(cve_id)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_repo_url ON repositories(url)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_repo_sha ON repositories(latest_commit_sha)"))
        conn.execute(text("CREATE INDEX IF NOT EXISTS idx_repo_created ON repositories(created_at)"))
        conn.commit()
    print("ç´¢å¼•åˆ›å»ºå®Œæˆ")

if __name__ == '__main__':
    add_indexes()
```

**å·¥ä½œé‡**: 2å°æ—¶

---

### 5.3 GPTæç¤ºè¯ä¼˜åŒ– â­â­â­

**é—®é¢˜**:
- å½“å‰æç¤ºè¯å¯èƒ½ä¸å¤Ÿç²¾ç¡®
- GPTå“åº”è´¨é‡æœ‰å¾…æå‡

**æ–¹æ¡ˆ**:

```python
# ä¼˜åŒ–æç¤ºè¯æ¨¡æ¿
OPTIMIZED_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç½‘ç»œå®‰å…¨åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹CVEæ¼æ´åŠå…¶POCä»£ç ï¼Œæä¾›ç²¾ç¡®çš„é£é™©è¯„ä¼°ã€‚

# ä»»åŠ¡è¦æ±‚
1. ç®€æ´æ˜äº†ï¼ˆé¿å…å†—ä½™ï¼‰
2. çªå‡ºå…³é”®æŠ€æœ¯ç»†èŠ‚
3. æä¾›å¯æ“ä½œçš„å»ºè®®

# è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
{{
    "risk": "ä½å±|ä¸­å±|é«˜å±|ä¸¥é‡",
    "type": "RCE|XSS|SQLæ³¨å…¥|æƒé™æå‡|...",
    "affected_version": "å—å½±å“ç‰ˆæœ¬",
    "exploit_difficulty": "ç®€å•|ä¸­ç­‰|å›°éš¾",
    "summary": "ä¸€å¥è¯æ€»ç»“ï¼ˆ20å­—å†…ï¼‰",
    "technical_details": "æŠ€æœ¯ç»†èŠ‚ï¼ˆ50å­—å†…ï¼‰",
    "mitigation": "ç¼“è§£æªæ–½ï¼ˆ30å­—å†…ï¼‰"
}}

# è¾“å…¥æ•°æ®
## CVEä¿¡æ¯
{cve_info}

## POCä»£ç 
{poc_code}

# é‡è¦æç¤º
- ä»…è¿”å›JSONï¼Œæ— å…¶ä»–å†…å®¹
- ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
- riskå¿…é¡»æ˜¯å››ä¸ªç­‰çº§ä¹‹ä¸€
"""

def ask_gpt_optimized(cve_info: Dict, poc_code: str) -> Optional[Dict]:
    """ä¼˜åŒ–çš„GPTè°ƒç”¨"""
    prompt = OPTIMIZED_PROMPT_TEMPLATE.format(
        cve_info=json.dumps(cve_info, indent=2),
        poc_code=poc_code[:5000]  # é™åˆ¶é•¿åº¦
    )

    # ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°
    data = {
        "model": get_config('GPT_MODEL'),
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,  # é™ä½éšæœºæ€§ï¼Œæå‡ä¸€è‡´æ€§
        "max_tokens": 500,   # é™åˆ¶è¾“å‡ºé•¿åº¦
        "response_format": {"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡ºï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    }

    # ... å…¶ä½™é€»è¾‘
```

**å·¥ä½œé‡**: 4å°æ—¶

---

## ğŸ“… å¼€å‘æ—¶é—´è¡¨

### è¿‘æœŸ (1ä¸ªæœˆå†…)

| å‘¨æ¬¡ | ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡å·¥æ—¶ |
|------|------|--------|---------|
| **Week 1** | ç›‘æ§é›†æˆåˆ°ä¸»æµç¨‹ | P0 | 2h |
| **Week 1** | å¹¶å‘å¤„ç†ä¼˜åŒ– | P0 | 8h |
| **Week 1** | å¼‚å¸¸æ¢å¤ä¸æ–­ç‚¹ç»­ä¼  | P0 | 4h |
| **Week 2** | å¢é‡æ›´æ–°æœºåˆ¶ | P0 | 2h |
| **Week 2** | å¤šæ¸ é“é€šçŸ¥æ”¯æŒ | P1 | 16h |
| **Week 3** | æ•°æ®å¯è§†åŒ–é¢æ¿ | P1 | 16h |
| **Week 4** | APIæ¥å£å¼€å‘ | P1 | 8h |
| **Week 4** | æ•°æ®å¯¼å‡ºåŠŸèƒ½ | P1 | 4h |

### ä¸­æœŸ (2-3ä¸ªæœˆ)

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡å·¥æ—¶ |
|------|--------|---------|
| ç¼“å­˜æœºåˆ¶ | P2 | 8h |
| æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– | P2 | 2h |
| GPTæç¤ºè¯ä¼˜åŒ– | P2 | 4h |
| æ—¥å¿—ç³»ç»Ÿä¼˜åŒ– | P2 | 4h |
| é…ç½®ç®¡ç†ä¼˜åŒ– | P2 | 4h |

### é•¿æœŸ (3-6ä¸ªæœˆ)

| ä»»åŠ¡ | ä¼˜å…ˆçº§ |
|------|--------|
| æ¼æ´é¢„æµ‹æ¨¡å‹ | P3 |
| è‡ªåŠ¨åŒ–ä¿®å¤å»ºè®® | P3 |
| ç¤¾åŒºè´¡çŒ®æœºåˆ¶ | P3 |
| å¤šè¯­è¨€æ”¯æŒ | P3 |

---

## ğŸ¯ å…³é”®é‡Œç¨‹ç¢‘

### v2.1 (1ä¸ªæœˆå)
- âœ… ç›‘æ§å…¨é¢é›†æˆ
- âœ… å¹¶å‘å¤„ç†ä¸Šçº¿
- âœ… å¼‚å¸¸æ¢å¤æœºåˆ¶
- âœ… å¢é‡æ›´æ–°æ¨¡å¼

**é¢„æœŸæ•ˆæœ**:
- å¤„ç†é€Ÿåº¦æå‡ **5-10å€**
- ç¨³å®šæ€§æ˜¾è‘—æå‡
- è¿è¡Œæ—¶é—´: 15åˆ†é’Ÿå®Œæˆå…¨é‡æ‰«æ

### v2.5 (2ä¸ªæœˆå)
- âœ… å¤šæ¸ é“é€šçŸ¥
- âœ… æ•°æ®å¯è§†åŒ–é¢æ¿
- âœ… RESTful API
- âœ… æ•°æ®å¯¼å‡º

**é¢„æœŸæ•ˆæœ**:
- ç”¨æˆ·ä½“éªŒå¤§å¹…æå‡
- æ•°æ®åˆ©ç”¨ç‡æå‡
- æ”¯æŒç¬¬ä¸‰æ–¹é›†æˆ

### v3.0 (3ä¸ªæœˆå)
- âœ… ç¼“å­˜ç³»ç»Ÿ
- âœ… æ€§èƒ½å…¨é¢ä¼˜åŒ–
- âœ… ä¼ä¸šçº§ç‰¹æ€§

**é¢„æœŸæ•ˆæœ**:
- ç”Ÿäº§çº§ç¨³å®šæ€§
- æˆæœ¬é™ä½ 50%
- æ”¯æŒå¤§è§„æ¨¡éƒ¨ç½²

---

## ğŸ’¡ åˆ›æ–°æ€§åŠŸèƒ½ï¼ˆæ¢ç´¢ï¼‰

### 1. æ¼æ´é¢„æµ‹æ¨¡å‹
åŸºäºå†å²æ•°æ®ï¼Œé¢„æµ‹å“ªäº›ä»“åº“å¯èƒ½å‘å¸ƒPOC

### 2. è‡ªåŠ¨åŒ–ä¿®å¤å»ºè®®
åŸºäºPOCä»£ç ï¼Œè‡ªåŠ¨ç”Ÿæˆä¿®å¤å»ºè®®

### 3. ç¤¾åŒºè´¡çŒ®å¹³å°
å…è®¸å®‰å…¨ç ”ç©¶è€…æäº¤å’Œåˆ†äº«åˆ†æ

### 4. å¨èƒæƒ…æŠ¥è”åŠ¨
ä¸å…¶ä»–å¨èƒæƒ…æŠ¥å¹³å°é›†æˆ

---

## ğŸ“Š æˆåŠŸæŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | v2.1ç›®æ ‡ | v3.0ç›®æ ‡ |
|------|------|---------|---------|
| å¤„ç†é€Ÿåº¦ | 1.5å°æ—¶ | 15åˆ†é’Ÿ | 5åˆ†é’Ÿ |
| æˆåŠŸç‡ | ~95% | >98% | >99% |
| APIè°ƒç”¨æ•° | åŸºå‡† | -50% | -70% |
| GPTæˆæœ¬ | åŸºå‡† | -20% | -40% |
| ç›‘æ§è¦†ç›– | éƒ¨åˆ† | 100% | 100% |
| ç³»ç»Ÿå¯ç”¨æ€§ | - | 95% | 99% |

---

## ğŸ”„ è¿­ä»£åŸåˆ™

1. **å°æ­¥å¿«è·‘**: æ¯æ¬¡è¿­ä»£1-2å‘¨ï¼Œå¿«é€Ÿäº¤ä»˜
2. **æ•°æ®é©±åŠ¨**: åŸºäºç›‘æ§æ•°æ®æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘
3. **ç”¨æˆ·åé¦ˆ**: æŒç»­æ”¶é›†ä½¿ç”¨åé¦ˆ
4. **å…¼å®¹ä¼˜å…ˆ**: ä¿æŒå‘åå…¼å®¹
5. **æ–‡æ¡£åŒæ­¥**: ä»£ç å’Œæ–‡æ¡£åŒæ­¥æ›´æ–°

---

## ğŸ“š å‚è€ƒèµ„æº

- [Streamlitæ–‡æ¡£](https://docs.streamlit.io/)
- [FastAPIæ–‡æ¡£](https://fastapi.tiangolo.com/)
- [GitHub APIæ–‡æ¡£](https://docs.github.com/en/rest)
- [OpenAI APIæœ€ä½³å®è·µ](https://platform.openai.com/docs/guides/prompt-engineering)

---

**æ›´æ–°æ—¥æœŸ**: 2025-11-27
**ç»´æŠ¤è€…**: VulnWatchdog Team
**ç‰ˆæœ¬**: v1.0
